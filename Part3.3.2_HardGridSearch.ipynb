{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " **Task 3.3: Parameter Tuning Using Grid Search**\n",
    "\n",
    "The grid search systematically explored 7 parameters across three main pipeline stages:\n",
    "- **TFâ€“IDF Vectorization**\n",
    "    - `max_features`\n",
    "    - `ngram_range`\n",
    "    - `min_df`\n",
    "    - `max_df`\n",
    "- **SVD (Latent Semantic Analysis**\n",
    "    - `n_components`\n",
    "    - `svd_random_state`\n",
    "- **Hierarchical Clustering**\n",
    "    - `n_clusters`\n",
    "    - `linkage_method`\n"
   ],
   "id": "b0283d925ec575f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:13:26.095490Z",
     "start_time": "2025-09-26T09:13:20.766873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def full_clustering_pipeline(connection_string,\n",
    "                             table_name='songs',\n",
    "                             text_column='cleanTokens',\n",
    "                             # TF-IDF params\n",
    "                             max_features=9000,\n",
    "                             ngram_range=(1, 2),\n",
    "                             min_df=25,\n",
    "                             max_df=0.9,\n",
    "                             # SVD params\n",
    "                             n_components=120,\n",
    "                             svd_random_state=100,\n",
    "                             # Hierarchical params\n",
    "                             n_clusters=6,\n",
    "                             linkage_method='ward',\n",
    "                             cut_height=None,\n",
    "                             # options\n",
    "                             soft_assign=True,\n",
    "                             distance_metric='euclidean',\n",
    "                             visualize=True,\n",
    "                             table_show=False,\n",
    "                             save_excel=False):\n",
    "    \"\"\"\n",
    "    Full pipeline: load SimilarityData, TF-IDF, SVD, Hierarchical clustering, soft assignment, PCA visualization.\n",
    "    Returns: df, X_reduced, prob_df, silhouette_score, vectorizer, svd, linkage_matrix\n",
    "    \"\"\"\n",
    "    # =====  Load & preprocess =====\n",
    "    engine = create_engine(connection_string)\n",
    "    df = pd.read_sql(f\"\"\"\n",
    "        SELECT song_id, name, {text_column}, cleanGenre\n",
    "        FROM {table_name}\n",
    "        WHERE {text_column} IS NOT NULL\n",
    "    \"\"\", engine)\n",
    "\n",
    "    df[text_column] = df[text_column].apply(ast.literal_eval)\n",
    "    df['clean_text'] = df[text_column].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "\n",
    "    # =====  TF-IDF =====\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        lowercase=True,\n",
    "        strip_accents='unicode',\n",
    "        token_pattern=r'\\b\\w+\\b'\n",
    "    )\n",
    "    X_sparse = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "    # =====  Truncated SVD (LSA) =====\n",
    "    svd = TruncatedSVD(n_components=min(n_components, X_sparse.shape[1]),\n",
    "                       random_state=svd_random_state)\n",
    "    X_reduced = svd.fit_transform(X_sparse)\n",
    "\n",
    "    # =====  Hierarchical clustering =====\n",
    "    Z = linkage(X_reduced, method=linkage_method)\n",
    "\n",
    "    if cut_height:\n",
    "        clusters = fcluster(Z, t=cut_height, criterion='distance')\n",
    "    else:\n",
    "        clusters = fcluster(Z, t=n_clusters, criterion='maxclust')\n",
    "\n",
    "    df['hierarchical_label'] = clusters\n",
    "\n",
    "    # Approx silhouette score\n",
    "    sil_score = silhouette_score(X_reduced, clusters)\n",
    "\n",
    "    # =====  Soft assignments =====\n",
    "    prob_df = None\n",
    "    assigned_cluster = clusters\n",
    "    if soft_assign:\n",
    "        # Approximate centroids per cluster\n",
    "        centroids = []\n",
    "        for k in sorted(set(clusters)):\n",
    "            centroids.append(X_reduced[clusters == k].mean(axis=0))\n",
    "        centroids = np.vstack(centroids)\n",
    "\n",
    "        dists = cdist(X_reduced, centroids, metric=distance_metric)\n",
    "        probabilities = softmax(-dists, axis=1)\n",
    "        assigned_cluster = probabilities.argmax(axis=1) + 1  # match fcluster labels (1-based)\n",
    "        prob_df = pd.DataFrame(probabilities,\n",
    "                               columns=[f'Cluster_{i + 1}' for i in range(centroids.shape[0])],\n",
    "                               index=df.index)\n",
    "        prob_df['assigned_cluster'] = assigned_cluster\n",
    "\n",
    "    # =====  Visualization =====\n",
    "    if visualize:\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_reduced)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "                              c=assigned_cluster,\n",
    "                              cmap='tab10', alpha=0.8, s=40)\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.title(f'Hierarchical Clusters (k={n_clusters}) in 2D PCA')\n",
    "        plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Optional dendrogram\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        dendrogram(Z)\n",
    "        if cut_height:\n",
    "            plt.axhline(y=cut_height, c='red', lw=2, linestyle='--')\n",
    "        plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "        plt.show()\n",
    "\n",
    "    # =====  Top subgenres per cluster & Excel =====\n",
    "    if table_show or save_excel:\n",
    "        for cluster_id in sorted(df['hierarchical_label'].unique()):\n",
    "            cluster_songs = df[df['hierarchical_label'] == cluster_id]\n",
    "            total_songs = len(cluster_songs)\n",
    "            all_subgenres = cluster_songs['cleanGenre'].dropna().apply(lambda x: [g.strip() for g in x.split(',')])\n",
    "            all_subgenres_flat = [g for sublist in all_subgenres for g in sublist]\n",
    "            subgenre_counts = Counter(all_subgenres_flat)\n",
    "            top5_subgenres = subgenre_counts.most_common(5)\n",
    "            top5_summary = \", \".join([f\"{g} ({c}/{total_songs})\" for g, c in top5_subgenres])\n",
    "            print(f\"\\n=== Cluster {cluster_id} ({total_songs} songs) | Top subgenres: {top5_summary} ===\")\n",
    "\n",
    "            if save_excel:\n",
    "                filename = f\"cluster_{cluster_id}.xlsx\"\n",
    "                cluster_songs[['name', 'cleanGenre', 'hierarchical_label']].to_excel(filename, index=False)\n",
    "\n",
    "    return df, X_reduced, prob_df, sil_score, vectorizer, svd, Z\n"
   ],
   "id": "98956d5c1a1a41b4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T09:13:26.125926Z",
     "start_time": "2025-09-26T09:13:26.105031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clustering_grid_search(connection_string,\n",
    "                           max_features_list=[8000, 9000, 10000],\n",
    "                           ngram_range_list=[(1, 1), (1, 2)],\n",
    "                           min_df_list=[20, 25, 30],\n",
    "                           max_df_list=[0.8, 0.9, 1.0],\n",
    "                           n_components_list=[100, 120, 150],\n",
    "                           svd_random_state_list=[42, 100],\n",
    "                           n_clusters_list=[5, 6, 7],\n",
    "                           linkage_method_list=['ward', 'complete', 'average'],\n",
    "                           verbose=True):\n",
    "    \"\"\"\n",
    "    Perform a grid search over TF-IDF, SVD, and Hierarchical clustering params,\n",
    "    including max_df. Returns a pandas DataFrame sorted by silhouette score.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    results = []\n",
    "\n",
    "    for max_features, ngram_range, min_df, max_df, n_components, svd_rs, n_clusters, linkage_method in itertools.product(\n",
    "        max_features_list, ngram_range_list, min_df_list, max_df_list,\n",
    "        n_components_list, svd_random_state_list,\n",
    "        n_clusters_list, linkage_method_list):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Testing: max_features={max_features}, ngram_range={ngram_range}, \"\n",
    "                  f\"min_df={min_df}, max_df={max_df}, n_components={n_components}, \"\n",
    "                  f\"svd_random_state={svd_rs}, n_clusters={n_clusters}, \"\n",
    "                  f\"linkage_method={linkage_method}\")\n",
    "\n",
    "        try:\n",
    "            _, _, _, sil_score, _, _, _ = full_clustering_pipeline(\n",
    "                connection_string=connection_string,\n",
    "                max_features=max_features,\n",
    "                ngram_range=ngram_range,\n",
    "                min_df=min_df,\n",
    "                max_df=max_df,\n",
    "                n_components=n_components,\n",
    "                svd_random_state=svd_rs,\n",
    "                n_clusters=n_clusters,\n",
    "                linkage_method=linkage_method,\n",
    "                soft_assign=False,\n",
    "                visualize=False,\n",
    "                table_show=False,\n",
    "                save_excel=False\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                'max_features': max_features,\n",
    "                'ngram_range': ngram_range,\n",
    "                'min_df': min_df,\n",
    "                'max_df': max_df,\n",
    "                'n_components': n_components,\n",
    "                'svd_random_state': svd_rs,\n",
    "                'n_clusters': n_clusters,\n",
    "                'linkage_method': linkage_method,\n",
    "                'silhouette_score': sil_score\n",
    "            })\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"â†’ Silhouette score: {sil_score:.4f}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped combination due to error: {e}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(by='silhouette_score', ascending=False).reset_index(drop=True)\n",
    "    return results_df\n"
   ],
   "id": "afd88ae0c7eb4bce",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-26T09:13:26.139462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read from file\n",
    "with open(\"SQL_DB/db_config.txt\", \"r\") as f:\n",
    "    db_target = f.read().strip()\n",
    "\n",
    "conn_str = (f\"mssql+pyodbc://{db_target}?driver=ODBC+Driver+17+for+SQL+Server\")\n",
    "\n",
    "\n",
    "grid_results = clustering_grid_search(\n",
    "    connection_string=conn_str,\n",
    "    max_features_list=[8000, 9000,],           # different max_features\n",
    "    ngram_range_list=[(1,1) ,(1, 2)],             # unigram and bigram combinations\n",
    "    min_df_list=[15, 20, 30],                       # different minimum document frequency\n",
    "    max_df_list=[0.8,0.9],                    # different maximum document frequency\n",
    "    n_components_list=[ 30, 40,100],             # different SVD dimensions\n",
    "    svd_random_state_list=[42,100],               # different SVD random states\n",
    "    n_clusters_list=[ 5, 6, 7],                      # different hierarchical cluster numbers\n",
    "    linkage_method_list=['ward' , 'average']  # different linkage methods\n",
    ")\n",
    "\n",
    "display(grid_results)\n"
   ],
   "id": "c6a4a6d0943f296d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=42, n_clusters=5, linkage_method=ward\n",
      "â†’ Silhouette score: 0.1200\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=42, n_clusters=5, linkage_method=average\n",
      "â†’ Silhouette score: 0.2219\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=42, n_clusters=6, linkage_method=ward\n",
      "â†’ Silhouette score: 0.0547\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=42, n_clusters=6, linkage_method=average\n",
      "â†’ Silhouette score: 0.1977\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=42, n_clusters=7, linkage_method=ward\n",
      "â†’ Silhouette score: 0.0601\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=42, n_clusters=7, linkage_method=average\n",
      "â†’ Silhouette score: 0.1676\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=100, n_clusters=5, linkage_method=ward\n",
      "â†’ Silhouette score: 0.1147\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=100, n_clusters=5, linkage_method=average\n",
      "â†’ Silhouette score: 0.1997\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=100, n_clusters=6, linkage_method=ward\n",
      "â†’ Silhouette score: 0.0583\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=100, n_clusters=6, linkage_method=average\n",
      "â†’ Silhouette score: 0.1860\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=100, n_clusters=7, linkage_method=ward\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Best Combination Found\n",
    "| max_features | ngram_range | min_df | max_df | n_components | svd_random_state | n_clusters | linkage_method | silhouette_score |\n",
    "|--------------|-------------|--------|--------|---------------|------------------|------------|----------------|------------------|\n",
    "| 8000         | (1, 2)      | 15     | 0.8    | 40            | 42               | 6          | average        | 0.149912         |\n",
    "\n",
    "\n",
    "\n",
    " Conclusion : Hierarchical clustering provides better separation than KMeans in this setup, though score remain weakly defined."
   ],
   "id": "27bad6ce20fe6698"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
