{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " **Task 3.3: Parameter Tuning Using Grid Search**\n",
    "\n",
    "The grid search systematically explored 7 parameters across three main pipeline stages:\n",
    "- **TFâ€“IDF Vectorization**\n",
    "    - `max_features`\n",
    "    - `ngram_range`\n",
    "    - `min_df`\n",
    "    - `max_df`\n",
    "- **SVD (Latent Semantic Analysis**\n",
    "    - `n_components`\n",
    "    - `svd_random_state`\n",
    "- **Hierarchical Clustering**\n",
    "    - `n_clusters`\n",
    "    - `linkage_method`\n"
   ],
   "id": "b0283d925ec575f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T15:50:47.270626Z",
     "start_time": "2025-09-21T15:50:42.448102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def full_clustering_pipeline(connection_string,\n",
    "                             table_name='songs',\n",
    "                             text_column='cleanTokens',\n",
    "                             # TF-IDF params\n",
    "                             max_features=9000,\n",
    "                             ngram_range=(1, 2),\n",
    "                             min_df=25,\n",
    "                             max_df=0.9,\n",
    "                             # SVD params\n",
    "                             n_components=120,\n",
    "                             svd_random_state=100,\n",
    "                             # Hierarchical params\n",
    "                             n_clusters=6,\n",
    "                             linkage_method='ward',\n",
    "                             cut_height=None,\n",
    "                             # options\n",
    "                             soft_assign=True,\n",
    "                             distance_metric='euclidean',\n",
    "                             visualize=True,\n",
    "                             table_show=False,\n",
    "                             save_excel=False):\n",
    "    \"\"\"\n",
    "    Full pipeline: load SimilarityData, TF-IDF, SVD, Hierarchical clustering, soft assignment, PCA visualization.\n",
    "    Returns: df, X_reduced, prob_df, silhouette_score, vectorizer, svd, linkage_matrix\n",
    "    \"\"\"\n",
    "    # =====  Load & preprocess =====\n",
    "    engine = create_engine(connection_string)\n",
    "    df = pd.read_sql(f\"\"\"\n",
    "        SELECT song_id, name, {text_column}, cleanGenre\n",
    "        FROM {table_name}\n",
    "        WHERE {text_column} IS NOT NULL\n",
    "    \"\"\", engine)\n",
    "\n",
    "    df[text_column] = df[text_column].apply(ast.literal_eval)\n",
    "    df['clean_text'] = df[text_column].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "\n",
    "    # =====  TF-IDF =====\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        lowercase=True,\n",
    "        strip_accents='unicode',\n",
    "        token_pattern=r'\\b\\w+\\b'\n",
    "    )\n",
    "    X_sparse = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "    # =====  Truncated SVD (LSA) =====\n",
    "    svd = TruncatedSVD(n_components=min(n_components, X_sparse.shape[1]),\n",
    "                       random_state=svd_random_state)\n",
    "    X_reduced = svd.fit_transform(X_sparse)\n",
    "\n",
    "    # =====  Hierarchical clustering =====\n",
    "    Z = linkage(X_reduced, method=linkage_method)\n",
    "\n",
    "    if cut_height:\n",
    "        clusters = fcluster(Z, t=cut_height, criterion='distance')\n",
    "    else:\n",
    "        clusters = fcluster(Z, t=n_clusters, criterion='maxclust')\n",
    "\n",
    "    df['hierarchical_label'] = clusters\n",
    "\n",
    "    # Approx silhouette score\n",
    "    sil_score = silhouette_score(X_reduced, clusters)\n",
    "\n",
    "    # =====  Soft assignments =====\n",
    "    prob_df = None\n",
    "    assigned_cluster = clusters\n",
    "    if soft_assign:\n",
    "        # Approximate centroids per cluster\n",
    "        centroids = []\n",
    "        for k in sorted(set(clusters)):\n",
    "            centroids.append(X_reduced[clusters == k].mean(axis=0))\n",
    "        centroids = np.vstack(centroids)\n",
    "\n",
    "        dists = cdist(X_reduced, centroids, metric=distance_metric)\n",
    "        probabilities = softmax(-dists, axis=1)\n",
    "        assigned_cluster = probabilities.argmax(axis=1) + 1  # match fcluster labels (1-based)\n",
    "        prob_df = pd.DataFrame(probabilities,\n",
    "                               columns=[f'Cluster_{i + 1}' for i in range(centroids.shape[0])],\n",
    "                               index=df.index)\n",
    "        prob_df['assigned_cluster'] = assigned_cluster\n",
    "\n",
    "    # =====  Visualization =====\n",
    "    if visualize:\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_reduced)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "                              c=assigned_cluster,\n",
    "                              cmap='tab10', alpha=0.8, s=40)\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.title(f'Hierarchical Clusters (k={n_clusters}) in 2D PCA')\n",
    "        plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Optional dendrogram\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        dendrogram(Z)\n",
    "        if cut_height:\n",
    "            plt.axhline(y=cut_height, c='red', lw=2, linestyle='--')\n",
    "        plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "        plt.show()\n",
    "\n",
    "    # =====  Top subgenres per cluster & Excel =====\n",
    "    if table_show or save_excel:\n",
    "        for cluster_id in sorted(df['hierarchical_label'].unique()):\n",
    "            cluster_songs = df[df['hierarchical_label'] == cluster_id]\n",
    "            total_songs = len(cluster_songs)\n",
    "            all_subgenres = cluster_songs['cleanGenre'].dropna().apply(lambda x: [g.strip() for g in x.split(',')])\n",
    "            all_subgenres_flat = [g for sublist in all_subgenres for g in sublist]\n",
    "            subgenre_counts = Counter(all_subgenres_flat)\n",
    "            top5_subgenres = subgenre_counts.most_common(5)\n",
    "            top5_summary = \", \".join([f\"{g} ({c}/{total_songs})\" for g, c in top5_subgenres])\n",
    "            print(f\"\\n=== Cluster {cluster_id} ({total_songs} songs) | Top subgenres: {top5_summary} ===\")\n",
    "\n",
    "            if save_excel:\n",
    "                filename = f\"cluster_{cluster_id}.xlsx\"\n",
    "                cluster_songs[['name', 'cleanGenre', 'hierarchical_label']].to_excel(filename, index=False)\n",
    "\n",
    "    return df, X_reduced, prob_df, sil_score, vectorizer, svd, Z\n"
   ],
   "id": "98956d5c1a1a41b4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T15:50:47.307107Z",
     "start_time": "2025-09-21T15:50:47.284906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clustering_grid_search(connection_string,\n",
    "                           max_features_list=[8000, 9000, 10000],\n",
    "                           ngram_range_list=[(1, 1), (1, 2)],\n",
    "                           min_df_list=[20, 25, 30],\n",
    "                           max_df_list=[0.8, 0.9, 1.0],\n",
    "                           n_components_list=[100, 120, 150],\n",
    "                           svd_random_state_list=[42, 100],\n",
    "                           n_clusters_list=[5, 6, 7],\n",
    "                           linkage_method_list=['ward', 'complete', 'average'],\n",
    "                           verbose=True):\n",
    "    \"\"\"\n",
    "    Perform a grid search over TF-IDF, SVD, and Hierarchical clustering params,\n",
    "    including max_df. Returns a pandas DataFrame sorted by silhouette score.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    results = []\n",
    "\n",
    "    for max_features, ngram_range, min_df, max_df, n_components, svd_rs, n_clusters, linkage_method in itertools.product(\n",
    "        max_features_list, ngram_range_list, min_df_list, max_df_list,\n",
    "        n_components_list, svd_random_state_list,\n",
    "        n_clusters_list, linkage_method_list):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Testing: max_features={max_features}, ngram_range={ngram_range}, \"\n",
    "                  f\"min_df={min_df}, max_df={max_df}, n_components={n_components}, \"\n",
    "                  f\"svd_random_state={svd_rs}, n_clusters={n_clusters}, \"\n",
    "                  f\"linkage_method={linkage_method}\")\n",
    "\n",
    "        try:\n",
    "            _, _, _, sil_score, _, _, _ = full_clustering_pipeline(\n",
    "                connection_string=connection_string,\n",
    "                max_features=max_features,\n",
    "                ngram_range=ngram_range,\n",
    "                min_df=min_df,\n",
    "                max_df=max_df,\n",
    "                n_components=n_components,\n",
    "                svd_random_state=svd_rs,\n",
    "                n_clusters=n_clusters,\n",
    "                linkage_method=linkage_method,\n",
    "                soft_assign=False,\n",
    "                visualize=False,\n",
    "                table_show=False,\n",
    "                save_excel=False\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                'max_features': max_features,\n",
    "                'ngram_range': ngram_range,\n",
    "                'min_df': min_df,\n",
    "                'max_df': max_df,\n",
    "                'n_components': n_components,\n",
    "                'svd_random_state': svd_rs,\n",
    "                'n_clusters': n_clusters,\n",
    "                'linkage_method': linkage_method,\n",
    "                'silhouette_score': sil_score\n",
    "            })\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"â†’ Silhouette score: {sil_score:.4f}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped combination due to error: {e}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(by='silhouette_score', ascending=False).reset_index(drop=True)\n",
    "    return results_df\n"
   ],
   "id": "afd88ae0c7eb4bce",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T15:52:34.451886Z",
     "start_time": "2025-09-21T15:50:47.320073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conn_str = \"mssql+pyodbc://IVAN_PC\\\\SQLEXPRESS/TextMiningHA?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "\n",
    "grid_results = clustering_grid_search(\n",
    "    connection_string=conn_str,\n",
    "    max_features_list=[8000, 9000,],           # different max_features\n",
    "    ngram_range_list=[(1,1) ,(1, 2)],             # unigram and bigram combinations\n",
    "    min_df_list=[15, 20, 30],                       # different minimum document frequency\n",
    "    max_df_list=[0.8,0.9],                    # different maximum document frequency\n",
    "    n_components_list=[ 30, 40,100],             # different SVD dimensions\n",
    "    svd_random_state_list=[42,100],               # different SVD random states\n",
    "    n_clusters_list=[ 5, 6, 7],                      # different hierarchical cluster numbers\n",
    "    linkage_method_list=['ward' , 'average']  # different linkage methods\n",
    ")\n",
    "\n",
    "display(grid_results)\n"
   ],
   "id": "c6a4a6d0943f296d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=42, n_clusters=5, linkage_method=ward\n",
      "â†’ Silhouette score: 0.1200\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=42, n_clusters=5, linkage_method=average\n",
      "â†’ Silhouette score: 0.2219\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=42, n_clusters=6, linkage_method=ward\n",
      "â†’ Silhouette score: 0.0547\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=42, n_clusters=6, linkage_method=average\n",
      "â†’ Silhouette score: 0.1977\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=42, n_clusters=7, linkage_method=ward\n",
      "â†’ Silhouette score: 0.0601\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=42, n_clusters=7, linkage_method=average\n",
      "â†’ Silhouette score: 0.1676\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=100, n_clusters=5, linkage_method=ward\n",
      "â†’ Silhouette score: 0.1147\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=100, n_clusters=5, linkage_method=average\n",
      "â†’ Silhouette score: 0.1997\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=100, n_clusters=6, linkage_method=ward\n",
      "â†’ Silhouette score: 0.0583\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=100, n_clusters=6, linkage_method=average\n",
      "â†’ Silhouette score: 0.1860\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=100, n_clusters=7, linkage_method=ward\n",
      "â†’ Silhouette score: 0.0624\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=30, svd_random_state=100, n_clusters=7, linkage_method=average\n",
      "â†’ Silhouette score: 0.1741\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=40, svd_random_state=42, n_clusters=5, linkage_method=ward\n",
      "â†’ Silhouette score: 0.0308\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=40, svd_random_state=42, n_clusters=5, linkage_method=average\n",
      "â†’ Silhouette score: 0.1651\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=40, svd_random_state=42, n_clusters=6, linkage_method=ward\n",
      "â†’ Silhouette score: 0.0349\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=40, svd_random_state=42, n_clusters=6, linkage_method=average\n",
      "â†’ Silhouette score: 0.1471\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=40, svd_random_state=42, n_clusters=7, linkage_method=ward\n",
      "â†’ Silhouette score: 0.0373\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=40, svd_random_state=42, n_clusters=7, linkage_method=average\n",
      "â†’ Silhouette score: 0.1370\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=40, svd_random_state=100, n_clusters=5, linkage_method=ward\n",
      "â†’ Silhouette score: 0.1075\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=40, svd_random_state=100, n_clusters=5, linkage_method=average\n",
      "â†’ Silhouette score: 0.1429\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=40, svd_random_state=100, n_clusters=6, linkage_method=ward\n",
      "â†’ Silhouette score: 0.0488\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=40, svd_random_state=100, n_clusters=6, linkage_method=average\n",
      "â†’ Silhouette score: 0.1262\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=40, svd_random_state=100, n_clusters=7, linkage_method=ward\n",
      "â†’ Silhouette score: 0.0504\n",
      "\n",
      "Testing: max_features=8000, ngram_range=(1, 1), min_df=15, max_df=0.8, n_components=40, svd_random_state=100, n_clusters=7, linkage_method=average\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m conn_str = \u001B[33m\"\u001B[39m\u001B[33mmssql+pyodbc://IVAN_PC\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33mSQLEXPRESS/TextMiningHA?driver=ODBC+Driver+17+for+SQL+Server\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m grid_results = \u001B[43mclustering_grid_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconnection_string\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconn_str\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_features_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m8000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m9000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m           \u001B[49m\u001B[38;5;66;43;03m# different max_features\u001B[39;49;00m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mngram_range_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m             \u001B[49m\u001B[38;5;66;43;03m# unigram and bigram combinations\u001B[39;49;00m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmin_df_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                       \u001B[49m\u001B[38;5;66;43;03m# different minimum document frequency\u001B[39;49;00m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_df_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0.8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m0.9\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                    \u001B[49m\u001B[38;5;66;43;03m# different maximum document frequency\u001B[39;49;00m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_components_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m40\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m             \u001B[49m\u001B[38;5;66;43;03m# different SVD dimensions\u001B[39;49;00m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43msvd_random_state_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m42\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m               \u001B[49m\u001B[38;5;66;43;03m# different SVD random states\u001B[39;49;00m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_clusters_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m7\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                      \u001B[49m\u001B[38;5;66;43;03m# different hierarchical cluster numbers\u001B[39;49;00m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlinkage_method_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mward\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43maverage\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# different linkage methods\u001B[39;49;00m\n\u001B[32m     13\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m display(grid_results)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 30\u001B[39m, in \u001B[36mclustering_grid_search\u001B[39m\u001B[34m(connection_string, max_features_list, ngram_range_list, min_df_list, max_df_list, n_components_list, svd_random_state_list, n_clusters_list, linkage_method_list, verbose)\u001B[39m\n\u001B[32m     24\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTesting: max_features=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_features\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, ngram_range=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mngram_range\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     25\u001B[39m           \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmin_df=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmin_df\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, max_df=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_df\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, n_components=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_components\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     26\u001B[39m           \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33msvd_random_state=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msvd_rs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, n_clusters=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_clusters\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     27\u001B[39m           \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mlinkage_method=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlinkage_method\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m     _, _, _, sil_score, _, _, _ = \u001B[43mfull_clustering_pipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconnection_string\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconnection_string\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m        \u001B[49m\u001B[43mngram_range\u001B[49m\u001B[43m=\u001B[49m\u001B[43mngram_range\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmin_df\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmin_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_df\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_components\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_components\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     37\u001B[39m \u001B[43m        \u001B[49m\u001B[43msvd_random_state\u001B[49m\u001B[43m=\u001B[49m\u001B[43msvd_rs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlinkage_method\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlinkage_method\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[43m        \u001B[49m\u001B[43msoft_assign\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtable_show\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[43m        \u001B[49m\u001B[43msave_excel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[32m     44\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     46\u001B[39m     results.append({\n\u001B[32m     47\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mmax_features\u001B[39m\u001B[33m'\u001B[39m: max_features,\n\u001B[32m     48\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mngram_range\u001B[39m\u001B[33m'\u001B[39m: ngram_range,\n\u001B[32m   (...)\u001B[39m\u001B[32m     55\u001B[39m         \u001B[33m'\u001B[39m\u001B[33msilhouette_score\u001B[39m\u001B[33m'\u001B[39m: sil_score\n\u001B[32m     56\u001B[39m     })\n\u001B[32m     58\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m verbose:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 66\u001B[39m, in \u001B[36mfull_clustering_pipeline\u001B[39m\u001B[34m(connection_string, table_name, text_column, max_features, ngram_range, min_df, max_df, n_components, svd_random_state, n_clusters, linkage_method, cut_height, soft_assign, distance_metric, visualize, table_show, save_excel)\u001B[39m\n\u001B[32m     63\u001B[39m \u001B[38;5;66;03m# =====  Truncated SVD (LSA) =====\u001B[39;00m\n\u001B[32m     64\u001B[39m svd = TruncatedSVD(n_components=\u001B[38;5;28mmin\u001B[39m(n_components, X_sparse.shape[\u001B[32m1\u001B[39m]),\n\u001B[32m     65\u001B[39m                    random_state=svd_random_state)\n\u001B[32m---> \u001B[39m\u001B[32m66\u001B[39m X_reduced = \u001B[43msvd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_sparse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[38;5;66;03m# =====  Hierarchical clustering =====\u001B[39;00m\n\u001B[32m     69\u001B[39m Z = linkage(X_reduced, method=linkage_method)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    314\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    318\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[32m    319\u001B[39m         return_tuple = (\n\u001B[32m    320\u001B[39m             _wrap_data_with_container(method, data_to_wrap[\u001B[32m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[32m    321\u001B[39m             *data_to_wrap[\u001B[32m1\u001B[39m:],\n\u001B[32m    322\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:244\u001B[39m, in \u001B[36mTruncatedSVD.fit_transform\u001B[39m\u001B[34m(self, X, y)\u001B[39m\n\u001B[32m    239\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.n_components > X.shape[\u001B[32m1\u001B[39m]:\n\u001B[32m    240\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    241\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mn_components(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.n_components\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) must be <=\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    242\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m n_features(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mX.shape[\u001B[32m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m).\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    243\u001B[39m         )\n\u001B[32m--> \u001B[39m\u001B[32m244\u001B[39m     U, Sigma, VT = \u001B[43m_randomized_svd\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    245\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    246\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_components\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    247\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_iter\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    248\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_oversamples\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_oversamples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpower_iteration_normalizer\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpower_iteration_normalizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m        \u001B[49m\u001B[43mflip_sign\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    253\u001B[39m     U, VT = svd_flip(U, VT, u_based_decision=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    255\u001B[39m \u001B[38;5;28mself\u001B[39m.components_ = VT\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:568\u001B[39m, in \u001B[36m_randomized_svd\u001B[39m\u001B[34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001B[39m\n\u001B[32m    564\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m transpose:\n\u001B[32m    565\u001B[39m     \u001B[38;5;66;03m# this implementation is a bit faster with smaller shape[1]\u001B[39;00m\n\u001B[32m    566\u001B[39m     M = M.T\n\u001B[32m--> \u001B[39m\u001B[32m568\u001B[39m Q = \u001B[43m_randomized_range_finder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    569\u001B[39m \u001B[43m    \u001B[49m\u001B[43mM\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    570\u001B[39m \u001B[43m    \u001B[49m\u001B[43msize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_random\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    571\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_iter\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    572\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpower_iteration_normalizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpower_iteration_normalizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    573\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    574\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    576\u001B[39m \u001B[38;5;66;03m# project M to the (k + p) dimensional space using the basis vectors\u001B[39;00m\n\u001B[32m    577\u001B[39m B = Q.T @ M\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:355\u001B[39m, in \u001B[36m_randomized_range_finder\u001B[39m\u001B[34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001B[39m\n\u001B[32m    351\u001B[39m     Q, _ = normalizer(A.T @ Q)\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# Sample the range of A using by linear projection of Q\u001B[39;00m\n\u001B[32m    354\u001B[39m \u001B[38;5;66;03m# Extract an orthonormal basis\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m355\u001B[39m Q, _ = \u001B[43mqr_normalizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA\u001B[49m\u001B[43m \u001B[49m\u001B[43m@\u001B[49m\u001B[43m \u001B[49m\u001B[43mQ\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m Q\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\scipy\\linalg\\_decomp_qr.py:167\u001B[39m, in \u001B[36mqr\u001B[39m\u001B[34m(a, overwrite_a, lwork, mode, pivoting, check_finite)\u001B[39m\n\u001B[32m    164\u001B[39m     Q, = safecall(gor_un_gqr, \u001B[33m\"\u001B[39m\u001B[33mgorgqr/gungqr\u001B[39m\u001B[33m\"\u001B[39m, qr[:, :M], tau,\n\u001B[32m    165\u001B[39m                   lwork=lwork, overwrite_a=\u001B[32m1\u001B[39m)\n\u001B[32m    166\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m mode == \u001B[33m'\u001B[39m\u001B[33meconomic\u001B[39m\u001B[33m'\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m167\u001B[39m     Q, = \u001B[43msafecall\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgor_un_gqr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgorgqr/gungqr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtau\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlwork\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlwork\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    168\u001B[39m \u001B[43m                  \u001B[49m\u001B[43moverwrite_a\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    170\u001B[39m     t = qr.dtype.char\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\scipy\\linalg\\_decomp_qr.py:19\u001B[39m, in \u001B[36msafecall\u001B[39m\u001B[34m(f, name, *args, **kwargs)\u001B[39m\n\u001B[32m     17\u001B[39m     ret = f(*args, **kwargs)\n\u001B[32m     18\u001B[39m     kwargs[\u001B[33m'\u001B[39m\u001B[33mlwork\u001B[39m\u001B[33m'\u001B[39m] = ret[-\u001B[32m2\u001B[39m][\u001B[32m0\u001B[39m].real.astype(numpy.int_)\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m ret = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ret[-\u001B[32m1\u001B[39m] < \u001B[32m0\u001B[39m:\n\u001B[32m     21\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33millegal value in \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[33mth argument of internal \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     22\u001B[39m                      % (-ret[-\u001B[32m1\u001B[39m], name))\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Best Combination Found\n",
    "| max_features | ngram_range | min_df | max_df | n_components | svd_random_state | n_clusters | linkage_method | silhouette_score |\n",
    "|--------------|-------------|--------|--------|---------------|------------------|------------|----------------|------------------|\n",
    "| 8000         | (1, 2)      | 15     | 0.8    | 40            | 42               | 6          | average        | 0.149912         |\n",
    "\n",
    "\n",
    "\n",
    " Conclusion : Hierarchical clustering provides better separation than KMeans in this setup, though score remain weakly defined."
   ],
   "id": "27bad6ce20fe6698"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
