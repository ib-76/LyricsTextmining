{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-21T16:08:20.580081Z",
     "start_time": "2025-09-21T16:08:20.565164Z"
    }
   },
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sqlalchemy import create_engine\n",
    "import ast\n",
    "\n",
    "def full_lda_pipeline(connection_string,\n",
    "                      table_name='songs',\n",
    "                      text_column='cleanTokens',\n",
    "                      n_topics=5,\n",
    "                      n_clusters=5,\n",
    "                      max_features=5000,\n",
    "                      lda_random_state=42,\n",
    "                      kmeans_random_state=42):\n",
    "    # Load SimilarityData\n",
    "    engine = create_engine(connection_string)\n",
    "    df = pd.read_sql(f\"\"\"\n",
    "        SELECT song_id, name, {text_column}, cleanGenre\n",
    "        FROM {table_name}\n",
    "        WHERE {text_column} IS NOT NULL\n",
    "    \"\"\", engine)\n",
    "    df[text_column] = df[text_column].apply(ast.literal_eval)\n",
    "    df['clean_text'] = df[text_column].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "\n",
    "    # Bag-of-Words\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    X_counts = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "    # LDA\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=lda_random_state)\n",
    "    lda.fit(X_counts)\n",
    "    topic_vectors = lda.transform(X_counts)\n",
    "    df['assigned_topic'] = topic_vectors.argmax(axis=1)\n",
    "\n",
    "    # KMeans in topic space\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=kmeans_random_state)\n",
    "    kmeans.fit(topic_vectors)\n",
    "    df['kmeans_label'] = kmeans.labels_\n",
    "\n",
    "    return df, topic_vectors\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T16:08:20.630371Z",
     "start_time": "2025-09-21T16:08:20.594686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lda_grid_search(connection_string,\n",
    "                    n_topics_list=[5, 6],\n",
    "                    max_features_list=[5000, 10000],\n",
    "                    k_clusters_list=[5, 6],\n",
    "                    lda_random_state_list=[42],\n",
    "                    kmeans_random_state_list=[42],\n",
    "                    verbose=True):\n",
    "    \"\"\"\n",
    "    Grid search for LDA + KMeans with separate random states.\n",
    "    Returns a sorted pandas DataFrame with silhouette scores.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for n_topics, max_features, k_clusters, lda_rs, km_rs in itertools.product(\n",
    "            n_topics_list, max_features_list, k_clusters_list,\n",
    "            lda_random_state_list, kmeans_random_state_list):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Testing: n_topics={n_topics}, max_features={max_features}, \"\n",
    "                  f\"k_clusters={k_clusters}, lda_random_state={lda_rs}, kmeans_random_state={km_rs}\")\n",
    "\n",
    "        try:\n",
    "            df, topic_vectors = full_lda_pipeline(\n",
    "                connection_string,\n",
    "                n_topics=n_topics,\n",
    "                n_clusters=k_clusters,\n",
    "                max_features=max_features,\n",
    "                lda_random_state=lda_rs,\n",
    "                kmeans_random_state=km_rs\n",
    "            )\n",
    "\n",
    "            labels = df['kmeans_label']\n",
    "            sil_score = silhouette_score(topic_vectors, labels)\n",
    "\n",
    "            results.append({\n",
    "                'n_topics': n_topics,\n",
    "                'max_features': max_features,\n",
    "                'k_clusters': k_clusters,\n",
    "                'lda_random_state': lda_rs,\n",
    "                'kmeans_random_state': km_rs,\n",
    "                'silhouette_score': sil_score\n",
    "            })\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"→ Silhouette score: {sil_score:.3f}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped combination due to error: {e}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(by='silhouette_score', ascending=False).reset_index(drop=True)\n",
    "    return results_df\n"
   ],
   "id": "b2fe36c365a852e3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T16:09:34.334411Z",
     "start_time": "2025-09-21T16:08:20.642408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Example usage\n",
    "conn_str = \"mssql+pyodbc://IVAN_PC\\\\SQLEXPRESS/TextMiningHA?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "\n",
    "grid_results = lda_grid_search(\n",
    "    connection_string=conn_str,\n",
    "    n_topics_list=[ 5,6,7],\n",
    "    max_features_list=[10000],\n",
    "    k_clusters_list=[ 5,6],\n",
    "    lda_random_state_list=[42, 100],\n",
    "    kmeans_random_state_list=[20,30, 90]\n",
    ")\n",
    "\n",
    "display(grid_results)\n"
   ],
   "id": "9cc383a99f881f0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: n_topics=5, max_features=10000, k_clusters=5, lda_random_state=42, kmeans_random_state=20\n",
      "→ Silhouette score: 0.629\n",
      "\n",
      "Testing: n_topics=5, max_features=10000, k_clusters=5, lda_random_state=42, kmeans_random_state=30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[32m      2\u001B[39m conn_str = \u001B[33m\"\u001B[39m\u001B[33mmssql+pyodbc://IVAN_PC\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33mSQLEXPRESS/TextMiningHA?driver=ODBC+Driver+17+for+SQL+Server\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m grid_results = \u001B[43mlda_grid_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconnection_string\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconn_str\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_topics_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m7\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_features_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m10000\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mk_clusters_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m6\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlda_random_state_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m42\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkmeans_random_state_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m90\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m display(grid_results)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 23\u001B[39m, in \u001B[36mlda_grid_search\u001B[39m\u001B[34m(connection_string, n_topics_list, max_features_list, k_clusters_list, lda_random_state_list, kmeans_random_state_list, verbose)\u001B[39m\n\u001B[32m     19\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTesting: n_topics=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_topics\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, max_features=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_features\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     20\u001B[39m           \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mk_clusters=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk_clusters\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, lda_random_state=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlda_rs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, kmeans_random_state=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkm_rs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m     df, topic_vectors = \u001B[43mfull_lda_pipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconnection_string\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_topics\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_topics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mk_clusters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlda_random_state\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlda_rs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkmeans_random_state\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkm_rs\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     32\u001B[39m     labels = df[\u001B[33m'\u001B[39m\u001B[33mkmeans_label\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     33\u001B[39m     sil_score = silhouette_score(topic_vectors, labels)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 34\u001B[39m, in \u001B[36mfull_lda_pipeline\u001B[39m\u001B[34m(connection_string, table_name, text_column, n_topics, n_clusters, max_features, lda_random_state, kmeans_random_state)\u001B[39m\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# LDA\u001B[39;00m\n\u001B[32m     33\u001B[39m lda = LatentDirichletAllocation(n_components=n_topics, random_state=lda_random_state)\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m \u001B[43mlda\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_counts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     35\u001B[39m topic_vectors = lda.transform(X_counts)\n\u001B[32m     36\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33massigned_topic\u001B[39m\u001B[33m'\u001B[39m] = topic_vectors.argmax(axis=\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:674\u001B[39m, in \u001B[36mLatentDirichletAllocation.fit\u001B[39m\u001B[34m(self, X, y)\u001B[39m\n\u001B[32m    666\u001B[39m         \u001B[38;5;28mself\u001B[39m._em_step(\n\u001B[32m    667\u001B[39m             X[idx_slice, :],\n\u001B[32m    668\u001B[39m             total_samples=n_samples,\n\u001B[32m    669\u001B[39m             batch_update=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    670\u001B[39m             parallel=parallel,\n\u001B[32m    671\u001B[39m         )\n\u001B[32m    672\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    673\u001B[39m     \u001B[38;5;66;03m# batch update\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m674\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_em_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    675\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_update\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparallel\u001B[49m\n\u001B[32m    676\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    678\u001B[39m \u001B[38;5;66;03m# check perplexity\u001B[39;00m\n\u001B[32m    679\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m evaluate_every > \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m (i + \u001B[32m1\u001B[39m) % evaluate_every == \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:523\u001B[39m, in \u001B[36mLatentDirichletAllocation._em_step\u001B[39m\u001B[34m(self, X, total_samples, batch_update, parallel)\u001B[39m\n\u001B[32m    496\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"EM update for 1 iteration.\u001B[39;00m\n\u001B[32m    497\u001B[39m \n\u001B[32m    498\u001B[39m \u001B[33;03mupdate `component_` by batch VB or online VB.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    519\u001B[39m \u001B[33;03m    Unnormalized document topic distribution.\u001B[39;00m\n\u001B[32m    520\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    522\u001B[39m \u001B[38;5;66;03m# E-step\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m523\u001B[39m _, suff_stats = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_e_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    524\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcal_sstats\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_init\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparallel\u001B[49m\n\u001B[32m    525\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    527\u001B[39m \u001B[38;5;66;03m# M-step\u001B[39;00m\n\u001B[32m    528\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m batch_update:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:466\u001B[39m, in \u001B[36mLatentDirichletAllocation._e_step\u001B[39m\u001B[34m(self, X, cal_sstats, random_init, parallel)\u001B[39m\n\u001B[32m    464\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m parallel \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    465\u001B[39m     parallel = Parallel(n_jobs=n_jobs, verbose=\u001B[38;5;28mmax\u001B[39m(\u001B[32m0\u001B[39m, \u001B[38;5;28mself\u001B[39m.verbose - \u001B[32m1\u001B[39m))\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m results = \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_update_doc_distribution\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    468\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx_slice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    469\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mexp_dirichlet_component_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    470\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdoc_topic_prior_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    471\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_doc_update_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    472\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmean_change_tol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    473\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcal_sstats\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    474\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    475\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    476\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx_slice\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mgen_even_slices\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    477\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    479\u001B[39m \u001B[38;5;66;03m# merge result\u001B[39;00m\n\u001B[32m    480\u001B[39m doc_topics, sstats_list = \u001B[38;5;28mzip\u001B[39m(*results)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     73\u001B[39m warning_filters = warnings.filters\n\u001B[32m     74\u001B[39m iterable_with_config_and_warning_filters = (\n\u001B[32m     75\u001B[39m     (\n\u001B[32m     76\u001B[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001B[32m   (...)\u001B[39m\u001B[32m     80\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     81\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config_and_warning_filters\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   1984\u001B[39m     output = \u001B[38;5;28mself\u001B[39m._get_sequential_output(iterable)\n\u001B[32m   1985\u001B[39m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m1986\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[32m   1988\u001B[39m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[32m   1989\u001B[39m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[32m   1990\u001B[39m \u001B[38;5;66;03m# reused, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[32m   1991\u001B[39m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[32m   1992\u001B[39m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[32m   1993\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._lock:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1914\u001B[39m, in \u001B[36mParallel._get_sequential_output\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   1912\u001B[39m \u001B[38;5;28mself\u001B[39m.n_dispatched_batches += \u001B[32m1\u001B[39m\n\u001B[32m   1913\u001B[39m \u001B[38;5;28mself\u001B[39m.n_dispatched_tasks += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1914\u001B[39m res = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1915\u001B[39m \u001B[38;5;28mself\u001B[39m.n_completed_tasks += \u001B[32m1\u001B[39m\n\u001B[32m   1916\u001B[39m \u001B[38;5;28mself\u001B[39m.print_progress()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001B[39m, in \u001B[36m_FuncWrapper.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(**config), warnings.catch_warnings():\n\u001B[32m    146\u001B[39m     warnings.filters = warning_filters\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Text Mining\\LyricsTextMining\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:147\u001B[39m, in \u001B[36m_update_doc_distribution\u001B[39m\u001B[34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001B[39m\n\u001B[32m    143\u001B[39m \u001B[38;5;66;03m# The optimal phi_{dwk} is proportional to\u001B[39;00m\n\u001B[32m    144\u001B[39m \u001B[38;5;66;03m# exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).\u001B[39;00m\n\u001B[32m    145\u001B[39m norm_phi = np.dot(exp_doc_topic_d, exp_topic_word_d) + eps\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m doc_topic_d = exp_doc_topic_d * \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcnts\u001B[49m\u001B[43m \u001B[49m\u001B[43m/\u001B[49m\u001B[43m \u001B[49m\u001B[43mnorm_phi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexp_topic_word_d\u001B[49m\u001B[43m.\u001B[49m\u001B[43mT\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    148\u001B[39m \u001B[38;5;66;03m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001B[39;00m\n\u001B[32m    149\u001B[39m dirichlet_expectation_1d(doc_topic_d, doc_topic_prior, exp_doc_topic_d)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **LDA + KMeans Results**\n",
    "\n",
    "To further improve clustering quality, Latent Dirichlet Allocation (LDA) was combined with KMeans clustering.\n",
    "This approach learns latent topics from the corpus and then groups the documents based on their topic distributions.\n",
    "##  Best Combination Found\n",
    "| n_topics | max_features | k_clusters | lda_random_state | kmeans_random_state | silhouette_score |\n",
    "|----------|--------------|------------|------------------|----------------------|------------------|\n",
    "| 6        | 10000        | 6          | 42               | 30                   | 0.795476         |\n",
    "\n",
    "**Discussion**\n",
    "- The best silhouette score ≈ 0.78 is significantly higher than the scores obtained from:\n",
    "  - KMeans with TF–IDF + SVD (~0.085)\n",
    "  - Hierarchical clustering (~0.150)\n",
    "- This indicates that LDA topics provide a much more meaningful representation of the text than raw TF–IDF features reduced with SVD.\n",
    "- With 6 latent topics and 6 final clusters, the structure of the dataset is captured far more effectively.\n",
    "- The results are satisfactory and demonstrate that probabilistic topic modeling can uncover coherent groups in the music data that traditional vectorization methods failed to separate.\n",
    "\n",
    "\n"
   ],
   "id": "f577f0bb0e8e6636"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Multithreading / Multiprocessing\n",
    "\n",
    "During my assignment process, I ran three notebooks simultaneously, each performing a grid search for a different clustering method (KMeans, Hierarchical, LDA).\n",
    "As I added more parameters, the time required increased significantly.\n",
    "\n",
    "Implementing Parallel Processing could help me to be more time efficient\n",
    "- Each parameter combination is independent, so multiple evaluations could run at the same time.\n",
    "- By utilizing multiple CPU cores multithreading  would reduce total runtime.\n",
    "\n",
    "Conclusion: Since I have a **2-core CPU**, I could not fully implement and evaluate multithreading,\n",
    "Parallelization could have greatly sped up my grid searches and reduced the overall experimentation time.\n"
   ],
   "id": "4d065a7cfcf99e98"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
