{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-22T18:15:28.446985Z",
     "start_time": "2025-09-22T18:15:27.316610Z"
    }
   },
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sqlalchemy import create_engine\n",
    "import ast\n",
    "\n",
    "def full_lda_pipeline(connection_string,\n",
    "                      table_name='songs',\n",
    "                      text_column='cleanTokens',\n",
    "                      n_topics=5,\n",
    "                      n_clusters=5,\n",
    "                      max_features=5000,\n",
    "                      lda_random_state=42,\n",
    "                      kmeans_random_state=42):\n",
    "    # Load SimilarityData\n",
    "    engine = create_engine(connection_string)\n",
    "    df = pd.read_sql(f\"\"\"\n",
    "        SELECT song_id, name, {text_column}, cleanGenre\n",
    "        FROM {table_name}\n",
    "        WHERE {text_column} IS NOT NULL\n",
    "    \"\"\", engine)\n",
    "    df[text_column] = df[text_column].apply(ast.literal_eval)\n",
    "    df['clean_text'] = df[text_column].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "\n",
    "    # Bag-of-Words\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    X_counts = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "    # LDA\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=lda_random_state)\n",
    "    lda.fit(X_counts)\n",
    "    topic_vectors = lda.transform(X_counts)\n",
    "    df['assigned_topic'] = topic_vectors.argmax(axis=1)\n",
    "\n",
    "    # KMeans in topic space\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=kmeans_random_state)\n",
    "    kmeans.fit(topic_vectors)\n",
    "    df['kmeans_label'] = kmeans.labels_\n",
    "\n",
    "    return df, topic_vectors\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T18:15:28.494063Z",
     "start_time": "2025-09-22T18:15:28.465351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lda_grid_search(connection_string,\n",
    "                    n_topics_list=[5, 6],\n",
    "                    max_features_list=[5000, 10000],\n",
    "                    k_clusters_list=[5, 6],\n",
    "                    lda_random_state_list=[42],\n",
    "                    kmeans_random_state_list=[42],\n",
    "                    verbose=True):\n",
    "    \"\"\"\n",
    "    Grid search for LDA + KMeans with separate random states.\n",
    "    Returns a sorted pandas DataFrame with silhouette scores.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for n_topics, max_features, k_clusters, lda_rs, km_rs in itertools.product(\n",
    "            n_topics_list, max_features_list, k_clusters_list,\n",
    "            lda_random_state_list, kmeans_random_state_list):\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Testing: n_topics={n_topics}, max_features={max_features}, \"\n",
    "                  f\"k_clusters={k_clusters}, lda_random_state={lda_rs}, kmeans_random_state={km_rs}\")\n",
    "\n",
    "        try:\n",
    "            df, topic_vectors = full_lda_pipeline(\n",
    "                connection_string,\n",
    "                n_topics=n_topics,\n",
    "                n_clusters=k_clusters,\n",
    "                max_features=max_features,\n",
    "                lda_random_state=lda_rs,\n",
    "                kmeans_random_state=km_rs\n",
    "            )\n",
    "\n",
    "            labels = df['kmeans_label']\n",
    "            sil_score = silhouette_score(topic_vectors, labels)\n",
    "\n",
    "            results.append({\n",
    "                'n_topics': n_topics,\n",
    "                'max_features': max_features,\n",
    "                'k_clusters': k_clusters,\n",
    "                'lda_random_state': lda_rs,\n",
    "                'kmeans_random_state': km_rs,\n",
    "                'silhouette_score': sil_score\n",
    "            })\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"→ Silhouette score: {sil_score:.3f}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped combination due to error: {e}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(by='silhouette_score', ascending=False).reset_index(drop=True)\n",
    "    return results_df\n"
   ],
   "id": "b2fe36c365a852e3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-22T18:15:28.517615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read from file\n",
    "with open(\"db_config.txt\", \"r\") as f:\n",
    "    db_target = f.read().strip()\n",
    "\n",
    "conn_str = (f\"mssql+pyodbc://{db_target}?driver=ODBC+Driver+17+for+SQL+Server\")\n",
    "\n",
    "grid_results = lda_grid_search(\n",
    "    connection_string=conn_str,\n",
    "    n_topics_list=[ 5,6,7],\n",
    "    max_features_list=[10000],\n",
    "    k_clusters_list=[ 5,6],\n",
    "    lda_random_state_list=[42, 100],\n",
    "    kmeans_random_state_list=[20,30, 90]\n",
    ")\n",
    "\n",
    "display(grid_results)\n"
   ],
   "id": "9cc383a99f881f0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: n_topics=5, max_features=10000, k_clusters=5, lda_random_state=42, kmeans_random_state=20\n",
      "→ Silhouette score: 0.629\n",
      "\n",
      "Testing: n_topics=5, max_features=10000, k_clusters=5, lda_random_state=42, kmeans_random_state=30\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **LDA + KMeans Results**\n",
    "\n",
    "To further improve clustering quality, Latent Dirichlet Allocation (LDA) was combined with KMeans clustering.\n",
    "This approach learns latent topics from the corpus and then groups the documents based on their topic distributions.\n",
    "##  Best Combination Found\n",
    "| n_topics | max_features | k_clusters | lda_random_state | kmeans_random_state | silhouette_score |\n",
    "|----------|--------------|------------|------------------|----------------------|------------------|\n",
    "| 6        | 10000        | 6          | 42               | 30                   | 0.795476         |\n",
    "\n",
    "**Discussion**\n",
    "- The best silhouette score ≈ 0.78 is significantly higher than the scores obtained from:\n",
    "  - KMeans with TF–IDF + SVD (~0.085)\n",
    "  - Hierarchical clustering (~0.150)\n",
    "- This indicates that LDA topics provide a much more meaningful representation of the text than raw TF–IDF features reduced with SVD.\n",
    "- With 6 latent topics and 6 final clusters, the structure of the dataset is captured far more effectively.\n",
    "- The results are satisfactory and demonstrate that probabilistic topic modeling can uncover coherent groups in the music data that traditional vectorization methods failed to separate.\n",
    "\n",
    "\n"
   ],
   "id": "f577f0bb0e8e6636"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Multithreading / Multiprocessing\n",
    "\n",
    "During my assignment process, I ran three notebooks simultaneously, each performing a grid search for a different clustering method (KMeans, Hierarchical, LDA).\n",
    "As I added more parameters, the time required increased significantly.\n",
    "\n",
    "Implementing Parallel Processing could help me to be more time efficient\n",
    "- Each parameter combination is independent, so multiple evaluations could run at the same time.\n",
    "- By utilizing multiple CPU cores multithreading  would reduce total runtime.\n",
    "\n",
    "Conclusion: Since I have a **2-core CPU**, I could not fully implement and evaluate multithreading,\n",
    "Parallelization could have greatly sped up my grid searches and reduced the overall experimentation time.\n"
   ],
   "id": "4d065a7cfcf99e98"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
