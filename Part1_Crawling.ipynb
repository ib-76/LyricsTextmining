{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Part 1: Data Acquisition and Extraction\n",
    "Task 1.1 Crawl and Save**\n",
    "\n",
    "In the notebook cell, I logged into the LyricsDB site using the provided credentials and the site’s IP address. To confirm successful login, I checked for the presence of \"LyricsDB\" in the HTML of the response.\n",
    "\n",
    "The lyrics were retrieved by sending programmatic HTTP requests to the site’s API using a session authenticated with the login credentials. Each page of songs was requested via a POST call, which returned JSON data.\n",
    "\n",
    "Finally, the songs retrieved from all pages were saved into a CSV file in the project’s dedicated folder, Songs, named crawled_songs.csv. Since I needed to use all songs for NLP analysis, I scraped all available songs from the site."
   ],
   "id": "7b4a639e0a5df13a"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-20T19:02:52.865168Z",
     "start_time": "2025-09-20T19:02:30.992128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def scrape_songs(max_pages=120):\n",
    "    \"\"\"\n",
    "    Scrape songs from LyricsDB and save them to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        max_pages (int): Maximum number of pages to scrape.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------\n",
    "    # User credentials\n",
    "    # ------------------------------\n",
    "    username = 'ivan.borg.i84578@mcast.edu.mt'\n",
    "    password = '0210JjtzUvKc'\n",
    "    base_url = 'http://23.94.19.185'\n",
    "\n",
    "    # ------------------------------\n",
    "    # Start session\n",
    "    # ------------------------------\n",
    "    session = requests.Session()\n",
    "    session.auth = (username, password)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Verify login\n",
    "    # ------------------------------\n",
    "    home_resp = session.get(base_url)\n",
    "    if \"LyricsDB\" not in home_resp.text:\n",
    "        print(\"Login failed — LyricsDB not found.\")\n",
    "        return\n",
    "\n",
    "    print(\"Logged in successfully — LyricsDB detected!\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Scrape pages\n",
    "    # ------------------------------\n",
    "    all_songs_url = f'{base_url}/all_songs.php'\n",
    "    all_songs = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        payload = {\"page\": page}\n",
    "        resp = session.post(all_songs_url, json=payload)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        data = resp.json()\n",
    "        songs = data.get('songs', [])\n",
    "        total = data.get('total', 0)\n",
    "\n",
    "        if not songs:\n",
    "            print(f\"No songs found on page {page}. Stopping...\")\n",
    "            break\n",
    "\n",
    "        all_songs.extend(songs)\n",
    "        print(f\"Scraping page {page}: {len(all_songs)} songs collected.\")\n",
    "\n",
    "        if total and len(all_songs) >= total:\n",
    "            print(\"All songs scraped successfully.\")\n",
    "            break\n",
    "\n",
    "    # ------------------------------\n",
    "    # Save to CSV (once after all pages)\n",
    "    # ------------------------------\n",
    "    csv_filename = \"Songs/crawled_songs.csv\"\n",
    "    if all_songs:\n",
    "        headers = all_songs[0].keys()\n",
    "        os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "\n",
    "        with open(csv_filename, mode='w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_songs)\n",
    "\n",
    "        print(f\"Saved {len(all_songs)} songs to {csv_filename}\")\n",
    "    else:\n",
    "        print(\"No songs found to save\")\n",
    "\n",
    "    return None\n",
    "\n",
    "    # ------------------------------\n",
    "    # execute function\n",
    "    # ------------------------------\n",
    "scrape_songs(max_pages=100)\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in successfully — LyricsDB detected!\n",
      "Scraping page 1: 20 songs collected.\n",
      "Scraping page 2: 40 songs collected.\n",
      "Scraping page 3: 60 songs collected.\n",
      "Scraping page 4: 80 songs collected.\n",
      "Scraping page 5: 100 songs collected.\n",
      "Scraping page 6: 120 songs collected.\n",
      "Scraping page 7: 140 songs collected.\n",
      "Scraping page 8: 160 songs collected.\n",
      "Scraping page 9: 180 songs collected.\n",
      "Scraping page 10: 200 songs collected.\n",
      "Scraping page 11: 220 songs collected.\n",
      "Scraping page 12: 240 songs collected.\n",
      "Scraping page 13: 260 songs collected.\n",
      "Scraping page 14: 280 songs collected.\n",
      "Scraping page 15: 300 songs collected.\n",
      "Scraping page 16: 320 songs collected.\n",
      "Scraping page 17: 340 songs collected.\n",
      "Scraping page 18: 360 songs collected.\n",
      "Scraping page 19: 380 songs collected.\n",
      "Scraping page 20: 400 songs collected.\n",
      "Scraping page 21: 420 songs collected.\n",
      "Scraping page 22: 440 songs collected.\n",
      "Scraping page 23: 460 songs collected.\n",
      "Scraping page 24: 480 songs collected.\n",
      "Scraping page 25: 500 songs collected.\n",
      "Scraping page 26: 520 songs collected.\n",
      "Scraping page 27: 540 songs collected.\n",
      "Scraping page 28: 560 songs collected.\n",
      "Scraping page 29: 580 songs collected.\n",
      "Scraping page 30: 600 songs collected.\n",
      "Scraping page 31: 620 songs collected.\n",
      "Scraping page 32: 640 songs collected.\n",
      "Scraping page 33: 660 songs collected.\n",
      "Scraping page 34: 680 songs collected.\n",
      "Scraping page 35: 700 songs collected.\n",
      "Scraping page 36: 720 songs collected.\n",
      "Scraping page 37: 740 songs collected.\n",
      "Scraping page 38: 760 songs collected.\n",
      "Scraping page 39: 780 songs collected.\n",
      "Scraping page 40: 800 songs collected.\n",
      "Scraping page 41: 820 songs collected.\n",
      "Scraping page 42: 840 songs collected.\n",
      "Scraping page 43: 860 songs collected.\n",
      "Scraping page 44: 880 songs collected.\n",
      "Scraping page 45: 900 songs collected.\n",
      "Scraping page 46: 920 songs collected.\n",
      "Scraping page 47: 940 songs collected.\n",
      "Scraping page 48: 960 songs collected.\n",
      "Scraping page 49: 980 songs collected.\n",
      "Scraping page 50: 1000 songs collected.\n",
      "Scraping page 51: 1020 songs collected.\n",
      "Scraping page 52: 1040 songs collected.\n",
      "Scraping page 53: 1060 songs collected.\n",
      "Scraping page 54: 1080 songs collected.\n",
      "Scraping page 55: 1100 songs collected.\n",
      "Scraping page 56: 1120 songs collected.\n",
      "Scraping page 57: 1140 songs collected.\n",
      "Scraping page 58: 1160 songs collected.\n",
      "Scraping page 59: 1180 songs collected.\n",
      "Scraping page 60: 1200 songs collected.\n",
      "Scraping page 61: 1220 songs collected.\n",
      "Scraping page 62: 1240 songs collected.\n",
      "Scraping page 63: 1260 songs collected.\n",
      "Scraping page 64: 1280 songs collected.\n",
      "Scraping page 65: 1300 songs collected.\n",
      "Scraping page 66: 1320 songs collected.\n",
      "Scraping page 67: 1340 songs collected.\n",
      "Scraping page 68: 1360 songs collected.\n",
      "Scraping page 69: 1380 songs collected.\n",
      "Scraping page 70: 1400 songs collected.\n",
      "Scraping page 71: 1420 songs collected.\n",
      "Scraping page 72: 1440 songs collected.\n",
      "Scraping page 73: 1460 songs collected.\n",
      "Scraping page 74: 1480 songs collected.\n",
      "Scraping page 75: 1500 songs collected.\n",
      "Scraping page 76: 1520 songs collected.\n",
      "Scraping page 77: 1540 songs collected.\n",
      "Scraping page 78: 1558 songs collected.\n",
      "All songs scraped successfully.\n",
      "Saved 1558 songs to Songs/crawled_songs.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the next cell, since the retrieved songs were still contained in the all_songs csv, I opted to store them in an SQL Express database called TextMiningHA for more convenient data manipulation. Dynamically, I created a table called songs and, from all the scraped information, used only the columns song_id, track_name, artists, lyrics, and genre. All extracted data were saved into their respective columns in the database.\n",
    "\n",
    "Additionally, I added three more columns for NLP processing: cleanGenre, tokenized, and cleanTokens. All processed data generated in the next project tasks will be saved into their respective columns accordingly."
   ],
   "id": "5582160972cc2f81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T19:02:56.592234Z",
     "start_time": "2025-09-20T19:02:52.887107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # Connect to SQL Server\n",
    "    conn = pyodbc.connect(\n",
    "        r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "        r'SERVER=IVAN_PC\\SQLEXPRESS;'\n",
    "        r'DATABASE=TextMiningHA;'\n",
    "        r'Trusted_Connection=yes;'\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create the \"songs\" table if it doesn't already exist\n",
    "    cursor.execute('''\n",
    "    IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='songs' AND xtype='U')\n",
    "    CREATE TABLE songs (\n",
    "        song_id NVARCHAR(35) PRIMARY KEY,  -- unique song identifier\n",
    "        name NVARCHAR(100),                -- track name\n",
    "        artist NVARCHAR(255),              -- artist name(s)\n",
    "        lyrics NVARCHAR(MAX),              -- lyrics text\n",
    "        genre NVARCHAR(MAX),               -- original genre\n",
    "        cleanGenre NVARCHAR(MAX),          -- cleaned genre for NLP\n",
    "        tokenised NVARCHAR(MAX),           -- tokenized lyrics\n",
    "        cleanTokens NVARCHAR(MAX)          -- cleaned tokens for NLP\n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    print(\"Connection successful and table ready.\")\n",
    "\n",
    "except pyodbc.Error as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "\n",
    "# Counter for inserted rows\n",
    "rows_added = 0\n",
    "\n",
    "# Load songs from CSV file in project root\n",
    "try:\n",
    "    df = pd.read_csv(\"Songs/crawled_songs.csv\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        song_id = str(row.get('song_id')) if pd.notna(row.get('song_id')) else None\n",
    "        name = str(row.get('track_name')) if pd.notna(row.get('track_name')) else None\n",
    "        artist = str(row.get('artists')) if pd.notna(row.get('artists')) else None\n",
    "        lyrics = str(row.get('lyrics')) if pd.notna(row.get('lyrics')) else None\n",
    "        genre = str(row.get('genre')) if pd.notna(row.get('genre')) else None\n",
    "\n",
    "        # Check if the song already exists in the database\n",
    "        cursor.execute(\"SELECT 1 FROM songs WHERE song_id = ?\", (song_id,))\n",
    "        if cursor.fetchone() is None:\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO songs (song_id, name, artist, lyrics, genre) VALUES (?, ?, ?, ?, ?)\",\n",
    "                (song_id, name, artist, lyrics, genre)\n",
    "            )\n",
    "            rows_added += 1\n",
    "\n",
    "    # Commit after inserting all rows\n",
    "    conn.commit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting songs: {e}\")\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(f\"Inserted {rows_added} new songs.\\r\\nConnection closed.\")\n"
   ],
   "id": "b829e242ce2c13a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful and table ready.\n",
      "Inserted 0 new songs.\r\n",
      "Connection closed.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " In the next cell, I included commands to connect to the SQL database, as well as operations for managing the data. This included deleting table or columns when needed, and fetching specific songs by applying filters. Mainly used as testing tool.",
   "id": "b25bc4edc1d11dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T19:02:57.270546Z",
     "start_time": "2025-09-20T19:02:56.616887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "try:\n",
    "    engine = create_engine(\"mssql+pyodbc://IVAN_PC\\\\SQLEXPRESS/TextMiningHA?driver=ODBC+Driver+17+for+SQL+Server\")\n",
    "    print(\"Connected to SQL Server!\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Connection failed:\")\n",
    "    print(e)\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "try:\n",
    "    conn = engine.connect()\n",
    "    trans = conn.begin()  # start a transaction manually\n",
    "    try:\n",
    "        #conn.execute(text(\"UPDATE songs SET cleanTokens = NULL\"))       #clear cleanTokens Column\n",
    "        #conn.execute(text(\"DELETE FROM songs WHERE genre LIKE '%K%'\"))  #delete rows where genre contains char K\n",
    "        #conn.execute(text(\"DROP TABLE IF EXISTS songs\"))                #delete entire table\n",
    "        trans.commit()  # explicitly commit\n",
    "    except Exception:\n",
    "        trans.rollback()  # roll back if something fails inside\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()  # make sure the connection is closed\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ],
   "id": "c8e31613db552314",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SQL Server!\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
