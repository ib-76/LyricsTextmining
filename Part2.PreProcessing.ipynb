{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Part 2: Preprocessing and Exploration\n",
    "Task 2.1: Text Cleaning**<br>\n",
    " Analysis     : My goal is to clean the lyrics without removing too much, so that I can retain as much of the original meaning and expression as possible.<br>\n",
    "                  - Since lyrics use artistic language and emotion, I want to avoid over-cleaning. Performing the following cleaning:-<br>\n",
    "                  - Remove duplicated tokens, to prevent repetitive noise [tokens included in repetitive chorus].<br>\n",
    "                  - Remove numbers and filler/vocalization words commonly used in songs (e.g. yeah, yeh, ok, ohh, ahhh).<br>\n",
    "                  - Keep negation words and important verbs, as they carry sentiment and meaning.<br>\n",
    "                  - Lightly handle punctuation and stopwords, only removing those clearly adding noise while keeping stylistic elements.<br>\n",
    "                  - Use various Python tools (regular expressions, NLTK, etc.) to help visualize and decide what to remove or keep.<br>\n",
    "                  - Overall, my aim is to keep valuable tokens that capture the expressive nature of lyrics, while removing obvious noise.\n"
   ],
   "id": "c79bd2c860149157"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pyodbc\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import contractions\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import regex as re  # regex module for Unicode support\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"\n",
    "    Connect to the SQL Server database and return the connection object.\n",
    "    \"\"\"\n",
    "    # Connect to local SQL Express instance with TextMiningHA database\n",
    "    conn = pyodbc.connect(\n",
    "        r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "        r'SERVER=IVAN_PC\\SQLEXPRESS;'\n",
    "        r'DATABASE=TextMiningHA;'\n",
    "        r'Trusted_Connection=yes;'\n",
    "    )\n",
    "\n",
    "    return conn\n",
    "\n",
    "def looks_like_a_word(token):\n",
    "    \"\"\"\n",
    "    True if token looks like a real word (letters only from any language, length >=3)\n",
    "    \"\"\"\n",
    "    # Uses regex Unicode property to match letters only\n",
    "    return bool(re.fullmatch(r'\\p{L}{3,}', token))\n",
    "\n",
    "def vocabulary_POS(tokens, pos_prefixes=('NN', 'VB', 'JJ')):\n",
    "    \"\"\"\n",
    "    Keep tokens that are nouns, verbs, or adjectives (based on POS tag prefix).\n",
    "    \"\"\"\n",
    "    tagged = pos_tag(tokens)  # POS tag all tokens\n",
    "    return [word for word, tag in tagged if tag.startswith(pos_prefixes)]\n",
    "\n",
    "def tokenize_all_lyrics():\n",
    "    \"\"\"\n",
    "    Basic tokenization: fetch lyrics from DB, expand contractions,\n",
    "    lowercase, tokenize, and save raw tokens to DB.\n",
    "    \"\"\"\n",
    "    conn = connect_db()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch all songs\n",
    "    cursor.execute(\"SELECT song_id, name, lyrics FROM songs ORDER BY song_id\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:'\\w+)?\")  # Tokenizer keeping contractions\n",
    "\n",
    "    for idx, (song_id, name, lyrics) in enumerate(rows, start=1):\n",
    "        # Replace line breaks with space\n",
    "        basic_clean_lyrics = (lyrics or \"\").replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "        # Split repeated phrases by multiple spaces\n",
    "        phrases = [p.strip() for p in basic_clean_lyrics.split('    ') if p.strip()]\n",
    "        unique_phrases = list(dict.fromkeys(phrases))  # remove duplicate phrases\n",
    "        clean_double_lyrics = '    '.join(unique_phrases)\n",
    "\n",
    "        # Expand contractions and lowercase\n",
    "        expanded_lyrics = contractions.fix(clean_double_lyrics).lower()\n",
    "\n",
    "        # Tokenize lyrics\n",
    "        tokens = tokenizer.tokenize(expanded_lyrics)\n",
    "\n",
    "        # Save raw tokenized lyrics to DB\n",
    "        cursor.execute(\n",
    "            \"UPDATE songs SET tokenised = ? WHERE song_id = ?\",\n",
    "            json.dumps(tokens, ensure_ascii=False),\n",
    "            song_id\n",
    "        )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Basic tokenization process ready filled in tokenised column songsdb.\")\n",
    "\n",
    "\n",
    "def clean_lyrics_tokens(start_row, end_row, show_output=1):\n",
    "    \"\"\"\n",
    "    Domain-specific cleaning: remove stopwords, lyric-specific noise words,\n",
    "    numbers, very short words, repeated characters, then save cleaned tokens to DB.\n",
    "    No lemmatization and no POS tagging.\n",
    "    \"\"\"\n",
    "    conn = connect_db()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch songs from DB\n",
    "    cursor.execute(\"SELECT song_id, name, lyrics, tokenised FROM songs ORDER BY song_id\")\n",
    "    rows = cursor.fetchall()\n",
    "    selected_rows = rows[start_row - 1:end_row]  # select subset of rows\n",
    "\n",
    "    # Define stopwords plus lyric-specific noise words\n",
    "    base_stopwords = set(stopwords.words('english'))\n",
    "    base_stopwords |= {'intro','aah','aahh','cuz','yeah','yea','hey','til',\n",
    "                       'chorus','ohoho','oho','ooh','ohh','oohh','whoa','aawaah'}\n",
    "\n",
    "    # Keep important negations & pronouns\n",
    "    keep_words = {'not','no','never','nor','but','if','you','i','he','she','they','our','my','us'}\n",
    "    base_stopwords -= keep_words  # remove them from stopwords\n",
    "\n",
    "    for idx, (song_id, name, lyrics, tokenised_json) in enumerate(selected_rows, start=start_row):\n",
    "        ordered_lyrics = (lyrics or \"\").replace('\\r', ' ').replace('\\n', ' ')\n",
    "        tokens = json.loads(tokenised_json or \"[]\")  # parse JSON token list\n",
    "\n",
    "        # Map auxiliary verbs to \"be\"\n",
    "        aux_map = {'is':'be', 'are':'be', 'was':'be', 'were':'be', 'am':'be', 'been':'be', 'being':'be'}\n",
    "        tokens = [aux_map.get(t.lower(), t) for t in tokens]\n",
    "\n",
    "        cleaned_tokens = []\n",
    "        for t in tokens:\n",
    "            t = re.sub(r\"'s$\", '', t)                        # Remove trailing \"'s\"\n",
    "            if t in keep_words:\n",
    "                cleaned_tokens.append(t)                     # Keep important pronouns\n",
    "            elif t in base_stopwords:\n",
    "                continue                                     # Remove stopwords\n",
    "            elif any(char.isdigit() for char in t):\n",
    "                continue                                     # Remove numbers\n",
    "            elif len(t) <= 2:\n",
    "                continue                                     # Remove very short tokens\n",
    "            elif re.search(r'(.)\\1\\1', t):\n",
    "                continue                                     # Remove repeated characters\n",
    "            elif not looks_like_a_word(t):\n",
    "                continue                                     # Remove invalid tokens\n",
    "            else:\n",
    "                cleaned_tokens.append(t.lower())            # Keep lowercase token\n",
    "\n",
    "        # Save cleaned tokens to DB\n",
    "        cursor.execute(\n",
    "            \"UPDATE songs SET cleanTokens = ? WHERE song_id = ?\",\n",
    "            json.dumps(cleaned_tokens, ensure_ascii=False),\n",
    "            song_id\n",
    "        )\n",
    "\n",
    "        # Optional debug output\n",
    "        if show_output == 1:\n",
    "            print(f\"--- Song {idx}: {name} ---\")\n",
    "            print(\"Original lyrics:\")\n",
    "            print(ordered_lyrics)\n",
    "            print(\"\\nTokenized lyrics:\")\n",
    "            print(tokens)\n",
    "            print(\"\\n\" + \"-\" * 100 + \"\\n\")\n",
    "            print(\"Polished tokens:\")\n",
    "            print(cleaned_tokens)\n",
    "            print(\"\\n\" + \".\" * 100 + \"\\n\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "\n",
    "# The following functions populate the 'tokenised' and 'cleanTokens' columns in the database:\n",
    "# - 'tokenised' stores the raw tokenized lyrics.\n",
    "# - 'cleanTokens' stores the cleaned, processed tokens ready for NLP analysis.\n",
    "\n",
    "\n",
    "#Tokenize all lyrics (raw tokens)\n",
    "tokenize_all_lyrics()\n",
    "\n",
    "# Clean tokenized lyrics for NLP (removes stopwords, noise, short words, etc.)\n",
    "clean_lyrics_tokens(1, 1600, show_output=0)  # Set show_output=1 to view original, tokenized, and cleaned lyrics\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Task 2.2: Exploratory Analysis <br> PART1**<br>Using functions such as fetch_tokenized_lyrics(), plot_wordcloud(tokens), plot_top_word_frequencies(tokens), print(vocabulary_diversity(tokens)), plot_pos_distribution(tokens, top_n=20), and fetch_cleaned_tokens(), I explored the dataset in several ways. These tools helped me generate word clouds to visualize frequent terms, plot word frequency distributions, and measure vocabulary diversity to detect repetition or variation. Finally, part-of-speech tagging allowed me to analyze grammatical roles and compare distributions before and after cleaning for a clearer understanding of the datasetâ€™s structure.\n",
   "id": "2bca34eba3a16ef2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from wordcloud import WordCloud\n",
    "from nltk.probability import FreqDist\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------\n",
    "# Fetch tokenized lyrics from DB\n",
    "# ------------------------------\n",
    "def fetch_tokenized_lyrics():\n",
    "    \"\"\"\n",
    "    Connect to SQL Server and fetch all tokenized lyrics from the 'tokenised' column.\n",
    "    Returns a flat list of all tokens from all songs.\n",
    "    \"\"\"\n",
    "    # Connect to database\n",
    "    conn = pyodbc.connect(\n",
    "        r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "        r'SERVER=IVAN_PC\\SQLEXPRESS;'\n",
    "        r'DATABASE=TextMiningHA;'\n",
    "        r'Trusted_Connection=yes;'\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Query all non-empty tokenised lyrics\n",
    "    cursor.execute(\"SELECT tokenised FROM songs WHERE tokenised IS NOT NULL\")\n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    # Flatten all token lists into a single list\n",
    "    all_tokens = []\n",
    "    for row in rows:\n",
    "        try:\n",
    "            tokens = json.loads(row[0])  # Convert JSON string to Python list\n",
    "            all_tokens.extend(tokens)\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # Skip malformed JSON entries\n",
    "\n",
    "    return all_tokens\n",
    "\n",
    "# ------------------------------\n",
    "# Fetch cleaned tokens\n",
    "# ------------------------------\n",
    "def fetch_cleaned_tokens():\n",
    "    \"\"\"\n",
    "    Connect to SQL Server and fetch DISTINCT tokens from the 'cleanTokens' column.\n",
    "    Prints all unique tokens sorted alphabetically.\n",
    "    \"\"\"\n",
    "    conn = pyodbc.connect(\n",
    "        r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "        r'SERVER=IVAN_PC\\SQLEXPRESS;'\n",
    "        r'DATABASE=TextMiningHA;'\n",
    "        r'Trusted_Connection=yes;'\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Query all non-empty cleanTokens\n",
    "    cursor.execute(\"SELECT cleanTokens FROM songs WHERE cleanTokens IS NOT NULL\")\n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    clean_tokens = set()  # Use a set to keep unique tokens\n",
    "    for (cleanTokens,) in rows:\n",
    "         try:\n",
    "             tokens_list = json.loads(cleanTokens)\n",
    "             clean_tokens.update(tokens_list)  # Add tokens to the set\n",
    "         except json.JSONDecodeError:\n",
    "             pass  # Skip invalid JSON entries\n",
    "\n",
    "    # Print total number of distinct tokens and list them alphabetically\n",
    "    print(f\"\\nTotal distinct tokens: {len(clean_tokens)}\")\n",
    "    print(\" \".join(f\"'{token}'\" for token in sorted(clean_tokens, key=str)))\n",
    "\n",
    "# ------------------------------\n",
    "# Plot word cloud\n",
    "# ------------------------------\n",
    "def plot_wordcloud(tokens):\n",
    "    \"\"\"\n",
    "    Generate and display a WordCloud from a list of tokens.\n",
    "    \"\"\"\n",
    "    text_data = \" \".join(tokens)  # Convert token list to a single string\n",
    "    wc = WordCloud(width=900, height=650, background_color=\"white\").generate(text_data)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Plot top N word frequencies\n",
    "# ------------------------------\n",
    "def plot_top_word_frequencies(tokens, top_n=120):\n",
    "    \"\"\"\n",
    "    Plot the top N most frequent words in the token list using Seaborn barplot.\n",
    "    \"\"\"\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    most_common = freq_dist.most_common(top_n)\n",
    "\n",
    "    words, counts = zip(*most_common)\n",
    "\n",
    "    plt.figure(figsize=(30, 16))\n",
    "    sns.barplot(x=list(words), y=list(counts), hue=list(words), palette=\"viridis\", legend=False)\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.title(f\"Top {top_n} Most Frequent Words\")\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Plot POS tag distribution\n",
    "# ------------------------------\n",
    "def plot_pos_distribution(tokens, top_n=20):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of POS tags in the token list.\n",
    "    Displays a bar plot of the most common POS tags.\n",
    "    \"\"\"\n",
    "    tagged_tokens = pos_tag(tokens)  # Tag all tokens\n",
    "    pos_tags = [tag for word, tag in tagged_tokens]  # Extract tags\n",
    "    freq_dist = FreqDist(pos_tags)\n",
    "    most_common = freq_dist.most_common(top_n)\n",
    "\n",
    "    tags, counts = zip(*most_common)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=list(tags), y=list(counts), hue=list(tags), palette=\"coolwarm\", legend=False)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Top {top_n} POS Tag Distribution\")\n",
    "    plt.xlabel(\"POS Tags\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Vocabulary diversity metrics\n",
    "# ------------------------------\n",
    "def vocabulary_diversity(tokens):\n",
    "    \"\"\"\n",
    "    Compute basic vocabulary statistics:\n",
    "    - TTR: Type-Token Ratio\n",
    "    - Total tokens\n",
    "    - Unique tokens\n",
    "    - Hapax Legomena: words appearing once\n",
    "    - Dis Legomena: words appearing twice\n",
    "    \"\"\"\n",
    "    total_tokens = len(tokens)\n",
    "    unique_tokens = len(set(tokens))\n",
    "    ttr = unique_tokens / total_tokens if total_tokens else 0\n",
    "\n",
    "    freq_dist = Counter(tokens)\n",
    "    hapax_legomena = len([w for w in freq_dist if freq_dist[w] == 1])\n",
    "    dis_legomena = len([w for w in freq_dist if freq_dist[w] == 2])\n",
    "\n",
    "    return {\n",
    "        \"TTR\": ttr,\n",
    "        \"Total tokens\": total_tokens,\n",
    "        \"Unique tokens\": unique_tokens,\n",
    "        \"Hapax Legomena\": hapax_legomena,\n",
    "        \"Dis Legomena\": dis_legomena,\n",
    "    }\n",
    "\n",
    "# ------------------------------\n",
    "# POS-based vocabulary filter\n",
    "# ------------------------------\n",
    "def vocabulary_POS(tokens, pos_prefixes=('NN', 'VB', 'JJ')):\n",
    "    \"\"\"\n",
    "    Keep only tokens that are nouns, verbs, or adjectives (based on POS tags).\n",
    "    \"\"\"\n",
    "    tagged = pos_tag(tokens)\n",
    "    return [word for word, tag in tagged if tag.startswith(pos_prefixes)]\n",
    "\n",
    "# ------------------------------\n",
    "# Execute analysis functions\n",
    "# ------------------------------\n",
    "tokens = fetch_tokenized_lyrics()            # Fetch all raw tokenized lyrics\n",
    "plot_wordcloud(tokens)                       # generate word cloud\n",
    "plot_top_word_frequencies(tokens, top_n=30)  # plot top 30 words\n",
    "print(vocabulary_diversity(tokens))          # print vocabulary metrics\n",
    "plot_pos_distribution(tokens, top_n=20)      # POS tag distribution plot\n",
    "fetch_cleaned_tokens()                       # Fetch and print unique cleaned tokens\n"
   ],
   "id": "617b795a143e4558",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Task 2.2: Exploratory Analysis <br> PART2**<br>This following code in the next  helped me analyze and compare the dataset before and after cleaning by visualizing token reduction with a graph. It also highlighted the exact types of tokens that were removed from the dataset, giving a clear view of how preprocessing affected the text.",
   "id": "e6b8b96e5bfd4a3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import datacompy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "def analyze_row(row_number):\n",
    "    \"\"\"\n",
    "    Fetch a specific row from the 'songs' table by its row offset,\n",
    "    compare token counts before and after cleaning, and visualize the change.\n",
    "\n",
    "    Parameters:\n",
    "        row_number (int): 0-based offset of the row to fetch from the database.\n",
    "    \"\"\"\n",
    "    # --- Database connection ---\n",
    "    # Use SQLAlchemy engine for querying SQL Server\n",
    "    engine = create_engine(\n",
    "        \"mssql+pyodbc://IVAN_PC\\\\SQLEXPRESS/TextMiningHA?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "    )\n",
    "\n",
    "    # --- Query to fetch one row by offset ---\n",
    "    query = f\"\"\"\n",
    "    SELECT song_id, name, tokenised, cleanTokens\n",
    "    FROM songs\n",
    "    ORDER BY song_id\n",
    "    OFFSET {row_number} ROWS FETCH NEXT 1 ROWS ONLY\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(query, conn)\n",
    "\n",
    "    # If no row found, stop execution\n",
    "    if df.empty:\n",
    "        print(f\"No row found for offset {row_number}\")\n",
    "        return\n",
    "\n",
    "    # --- Convert JSON-like strings back to Python lists ---\n",
    "    df['tokenised'] = df['tokenised'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['cleanTokens'] = df['cleanTokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # --- Count tokens before and after cleaning ---\n",
    "    before_count = len(df['tokenised'][0])\n",
    "    after_count = len(df['cleanTokens'][0])\n",
    "\n",
    "    # --- Visualization: bar chart of reduction ---\n",
    "    plt.bar(['Before', 'After'], [before_count, after_count], color=['red', 'green'])\n",
    "    plt.ylabel('Token Count')\n",
    "    plt.title(f\"Token Reduction for '{df['name'][0]}' â€” Song number {row_number + 1}\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- Print summary statistics ---\n",
    "    print(f\"Before: {before_count} tokens\")\n",
    "    print(f\"After: {after_count} tokens\")\n",
    "    print(f\"Reduction: {before_count - after_count} tokens \"\n",
    "          f\"({(before_count - after_count) / before_count * 100:.1f}%)\")\n",
    "\n",
    "    # --- Prepare DataFrames for comparison ---\n",
    "    # Add artificial index to preserve token order\n",
    "    before_df = pd.DataFrame({'token': df['tokenised'][0]})\n",
    "    before_df['idx'] = before_df.index\n",
    "\n",
    "    after_df = pd.DataFrame({'token': df['cleanTokens'][0]})\n",
    "    after_df['idx'] = after_df.index\n",
    "\n",
    "    # --- Compare before vs after using datacompy ---\n",
    "    compare = datacompy.Compare(\n",
    "        before_df,\n",
    "        after_df,\n",
    "        join_columns=['idx'],       # Align on artificial index\n",
    "        df1_name='Before Cleaning',\n",
    "        df2_name='After Cleaning'\n",
    "    )\n",
    "    # Optional: uncomment for full report\n",
    "    # print(compare.report())\n",
    "\n",
    "    # --- Show which tokens were removed/added ---\n",
    "    removed = set(df['tokenised'][0]) - set(df['cleanTokens'][0])\n",
    "    added = set(df['cleanTokens'][0]) - set(df['tokenised'][0])\n",
    "\n",
    "    print(f\"Removed tokens: {removed}\\n\")\n",
    "    print(f\"Added tokens: {added}\")\n",
    "\n",
    "\n",
    "# --- Run analysis on 20 random songs ---\n",
    "# Pick 20 random row offsets between 1 and 100 (change range as needed)\n",
    "random_rows = random.sample(range(560, 900), 20)\n",
    "for row in random_rows:\n",
    "    analyze_row(row)\n"
   ],
   "id": "3c323e620513e84c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Final Thoughts:** My best tool for preparing tokens for TF-IDF was the **\"clean_lyrics_tokens(1,1600, show_output=0)\"** function located in the first cell. It allowed me to carefully reduce noise without aggressively cutting down the token set, maintaining a balance between token quantity and semantic richness. Using this function helped me refine my regex patterns and preprocessing steps.<br>A typical output shows the song number and name, the original lyrics, the raw tokens extracted from tokenization, and the polished tokens after applying regex rules and removing stopwords, numbers, and repeated characters. This process ensures that meaningful words are retained while noise is eliminated, producing a balanced token set suitable for TF-IDF and other NLP analyses.\n",
    "\n",
    "\n",
    "\n",
    "--- Song 1: How Can You Mend A Broken Heart ---\n",
    "Original lyrics:\n",
    "I can think of younger days when living for my life    Was everything a man could want to do    I could never see tomorrow,    But I was never told about the sorrow        And how can you mend a broken heart?    How can you stop the rain from falling down?    How can you stop the sun from shining?    What makes the world go round?    How can you mend this broken man?    How can a loser ever win?    Please help me mend my broken heart and let me live again        I can still feel the breeze that rustles through the trees    And misty memories of days gone by    We could never see tomorrow,    No one said a word about the sorrow        And how can you mend a broken heart?    How can you stop the rain from falling down?    How can you stop the sun from shining?    What makes the world go round?    And how can you mend this broken man?    How can a loser ever win?    Please help me mend my broken heart and let me live again        La la la la la la, la la la la    La la la la la la, la la la la        Please help me mend my broken heart and let me live again        Da da da da    Da da da da, da da da da da, da\n",
    "\n",
    "Tokenized lyrics:\n",
    "['i', 'can', 'think', 'of', 'younger', 'days', 'when', 'living', 'for', 'my', 'life', 'be', 'everything', 'a', 'man', 'could', 'want', 'to', 'do', 'i', 'could', 'never', 'see', 'tomorrow', 'but', 'i', 'be', 'never', 'told', 'about', 'the', 'sorrow', 'and', 'how', 'can', 'you', 'mend', 'a', 'broken', 'heart', 'how', 'can', 'you', 'stop', 'the', 'rain', 'from', 'falling', 'down', 'how', 'can', 'you', 'stop', 'the', 'sun', 'from', 'shining', 'what', 'makes', 'the', 'world', 'go', 'round', 'how', 'can', 'you', 'mend', 'this', 'broken', 'man', 'how', 'can', 'a', 'loser', 'ever', 'win', 'please', 'help', 'me', 'mend', 'my', 'broken', 'heart', 'and', 'let', 'me', 'live', 'again', 'i', 'can', 'still', 'feel', 'the', 'breeze', 'that', 'rustles', 'through', 'the', 'trees', 'and', 'misty', 'memories', 'of', 'days', 'gone', 'by', 'we', 'could', 'never', 'see', 'tomorrow', 'no', 'one', 'said', 'a', 'word', 'about', 'the', 'sorrow', 'and', 'how', 'can', 'you', 'mend', 'this', 'broken', 'man', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'la', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da', 'da']\n",
    "\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\n",
    "Polished tokens:\n",
    "['i', 'think', 'younger', 'days', 'living', 'my', 'life', 'everything', 'man', 'could', 'want', 'i', 'could', 'never', 'see', 'tomorrow', 'but', 'i', 'never', 'told', 'sorrow', 'you', 'mend', 'broken', 'heart', 'you', 'stop', 'rain', 'falling', 'you', 'stop', 'sun', 'shining', 'makes', 'world', 'round', 'you', 'mend', 'broken', 'man', 'loser', 'ever', 'win', 'please', 'help', 'mend', 'my', 'broken', 'heart', 'let', 'live', 'i', 'still', 'feel', 'breeze', 'rustles', 'trees', 'misty', 'memories', 'days', 'gone', 'could', 'never', 'see', 'tomorrow', 'no', 'one', 'said', 'word', 'sorrow', 'you', 'mend', 'broken', 'man']\n",
    "\n",
    "..................................................................................................................................................."
   ],
   "id": "6cb9af62241a99a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
